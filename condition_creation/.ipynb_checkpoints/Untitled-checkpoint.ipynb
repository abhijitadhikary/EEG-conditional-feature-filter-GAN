{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "\n",
    "def gen_images(locs, features, n_gridpoints, normalize=True,\n",
    "               augment=False, pca=False, std_mult=0.1, n_components=2, edgeless=False):\n",
    "    \"\"\"\n",
    "    Generates EEG images given electrode locations in 2D space and multiple feature values for each electrode\n",
    "\n",
    "    :param locs: An array with shape [n_electrodes, 2] containing X, Y\n",
    "                        coordinates for each electrode.\n",
    "    :param features: Feature matrix as [n_samples, n_features]\n",
    "                                Features are as columns.\n",
    "                                Features corresponding to each frequency band are concatenated.\n",
    "                                (alpha1, alpha2, ..., beta1, beta2,...)\n",
    "    :param n_gridpoints: Number of pixels in the output images\n",
    "    :param normalize:   Flag for whether to normalize each band over all samples\n",
    "    :param augment:     Flag for generating augmented images\n",
    "    :param pca:         Flag for PCA based data augmentation\n",
    "    :param std_mult     Multiplier for std of added noise\n",
    "    :param n_components: Number of components in PCA to retain for augmentation\n",
    "    :param edgeless:    If True generates edgeless images by adding artificial channels\n",
    "                        at four corners of the image with value = 0 (default=False).\n",
    "    :return:            Tensor of size [samples, colors, W, H] containing generated\n",
    "                        images.\n",
    "    \"\"\"\n",
    "    locs = np.array(locs)\n",
    "    features = features.reshape(np.shape(features)[0], np.shape(features)[1] * np.shape(features)[2])\n",
    "    feat_array_temp = []\n",
    "    nElectrodes = locs.shape[0]  # Number of electrodes\n",
    "    # Test whether the feature vector length is divisible by number of electrodes\n",
    "    assert features.shape[1] % nElectrodes == 0\n",
    "    n_colors = int(features.shape[1] / nElectrodes)\n",
    "    for c in range(n_colors):\n",
    "        feat_array_temp.append(features[:, c * nElectrodes: nElectrodes * (c + 1)])\n",
    "    if augment:\n",
    "        if pca:\n",
    "            for c in range(n_colors):\n",
    "                feat_array_temp[c] = augment_EEG(feat_array_temp[c], std_mult, pca=True, n_components=n_components)\n",
    "        else:\n",
    "            for c in range(n_colors):\n",
    "                feat_array_temp[c] = augment_EEG(feat_array_temp[c], std_mult, pca=False, n_components=n_components)\n",
    "    nSamples = features.shape[0]\n",
    "    # Interpolate the values\n",
    "    grid_x, grid_y = np.mgrid[\n",
    "                     min(locs[:, 0]):max(locs[:, 0]):n_gridpoints * 1j,\n",
    "                     min(locs[:, 1]):max(locs[:, 1]):n_gridpoints * 1j\n",
    "                     ]\n",
    "    temp_interp = []\n",
    "    for c in range(n_colors):\n",
    "        temp_interp.append(np.zeros([nSamples, n_gridpoints, n_gridpoints]))\n",
    "    # Generate edgeless images\n",
    "    if edgeless:\n",
    "        min_x, min_y = np.min(locs, axis=0)\n",
    "        max_x, max_y = np.max(locs, axis=0)\n",
    "        locs = np.append(locs, np.array([[min_x, min_y], [min_x, max_y], [max_x, min_y], [max_x, max_y]]), axis=0)\n",
    "        for c in range(n_colors):\n",
    "            feat_array_temp[c] = np.append(feat_array_temp[c], np.zeros((nSamples, 4)), axis=1)\n",
    "    # Interpolating\n",
    "    for i in range(nSamples):\n",
    "        for c in range(n_colors):\n",
    "            temp_interp[c][i, :, :] = griddata(locs, feat_array_temp[c][i, :], (grid_x, grid_y),\n",
    "                                               method='cubic', fill_value=np.nan)\n",
    "    #         print('Interpolating {0}/{1}\\r'.format(i+1, nSamples), end='\\r')\n",
    "\n",
    "    # Normalizing\n",
    "    for c in range(n_colors):\n",
    "        if normalize:\n",
    "            temp_interp[c][~np.isnan(temp_interp[c])] = \\\n",
    "                scale(temp_interp[c][~np.isnan(temp_interp[c])])\n",
    "        temp_interp[c] = np.nan_to_num(temp_interp[c])  # Replace nan with zero and inf with large finite numbers.\n",
    "    images = np.swapaxes(np.asarray(temp_interp), 0, 1)  # swap axes to have [samples, colors, W, H]\n",
    "\n",
    "    images = np.transpose(images, (0, 3, 2, 1))\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def make_frames(df, frame_duration):\n",
    "    '''\n",
    "    in: dataframe or array with all channels, frame duration in seconds\n",
    "    out: array of theta, alpha, beta averages for each probe for each time step\n",
    "        shape: (n-frames,m-probes,k-brainwave bands)\n",
    "    '''\n",
    "    Fs = 256.0\n",
    "    frame_length = Fs * frame_duration\n",
    "\n",
    "    frames = []\n",
    "    # print('df shape',np.shape(df))\n",
    "    for i in range(0, np.shape(df)[0]):\n",
    "        frame = []\n",
    "\n",
    "        for channel in range(0, np.shape(df)[1]):\n",
    "            snippet = df[i][channel]\n",
    "            # print(i, channel)\n",
    "            # f,Y =  get_fft(snippet)\n",
    "            # print (len(snippet))\n",
    "            theta, alpha, beta = theta_alpha_beta_averages(np.array(range(len(snippet))), snippet)\n",
    "            # print (theta, alpha, beta)\n",
    "            frame.append([theta, alpha, beta])\n",
    "\n",
    "        frames.append(frame)\n",
    "        if i == len(df) - 1:\n",
    "            print('===== %d end =====' % (i))\n",
    "    frames = np.array(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "def theta_alpha_beta_averages(f, Y):\n",
    "    theta_range = (4, 8)\n",
    "    alpha_range = (8, 13)\n",
    "    beta_range = (13, 30)\n",
    "    theta = Y[(f > theta_range[0]) & (f <= theta_range[1])].mean()\n",
    "    alpha = Y[(f > alpha_range[0]) & (f <= alpha_range[1])].mean()\n",
    "    beta = Y[(f > beta_range[0]) & (f <= beta_range[1])].mean()\n",
    "    return theta, alpha, beta\n",
    "\n",
    "\n",
    "def convert_time_to_frequency(data_time):\n",
    "    # convert from time to frequency domain\n",
    "    data_time = np.transpose(data_time, (0, 2, 1))\n",
    "    num_exp, num_ch, rate = data_time.shape\n",
    "\n",
    "    data_freq = []\n",
    "    # time-frequency convertion on all the EEG signals\n",
    "    for i in range(num_exp):\n",
    "        spectrum = []\n",
    "        for ch in range(num_ch):\n",
    "            time_domain = data_time[i][ch]\n",
    "            f, magnitude = get_fft(time_domain)\n",
    "            spectrum.append(magnitude)\n",
    "        data_freq.append(spectrum)\n",
    "\n",
    "    return np.array(data_freq)\n",
    "\n",
    "\n",
    "def get_fft(snippet):\n",
    "    Fs = 256.0;  # sampling rate\n",
    "    y = snippet\n",
    "    n = len(y)  # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n / Fs\n",
    "    frq = k / T  # two sides frequency range\n",
    "    frq = frq[range(int(n / 2))]  # one side frequency range\n",
    "\n",
    "    Y = np.fft.fft(y) / n  # fft computing and normalization\n",
    "    Y = Y[range(int(n / 2))]\n",
    "    return frq, abs(Y)\n",
    "\n",
    "\n",
    "def get_2d_electrode_locations():\n",
    "    # Load electrode locations\n",
    "    locs = loadmat('Neuroscan_locs_orig.mat')\n",
    "    locs_3d = locs['A']\n",
    "    locs_2d = []\n",
    "    # Convert to 2D\n",
    "    for e in locs_3d:\n",
    "        locs_2d.append(azim_proj(e))\n",
    "    return np.array(locs_2d)\n",
    "\n",
    "\n",
    "def azim_proj(pos):\n",
    "    \"\"\"\n",
    "    Computes the Azimuthal Equidistant Projection of input point in 3D Cartesian Coordinates.\n",
    "    Imagine a plane being placed against (tangent to) a globe. If\n",
    "    a light source inside the globe projects the graticule onto\n",
    "    the plane the result would be a planar, or azimuthal, map\n",
    "    projection.\n",
    "\n",
    "    :param pos: position in 3D Cartesian coordinates\n",
    "    :return: projected coordinates using Azimuthal Equidistant Projection\n",
    "    \"\"\"\n",
    "    [r, elev, az] = cart2sph(pos[0], pos[1], pos[2])\n",
    "    return pol2cart(az, np.pi / 2 - elev)\n",
    "\n",
    "\n",
    "def cart2sph(x, y, z):\n",
    "    \"\"\"\n",
    "    Transform Cartesian coordinates to spherical\n",
    "    :param x: X coordinate\n",
    "    :param y: Y coordinate\n",
    "    :param z: Z coordinate\n",
    "    :return: radius, elevation, azimuth\n",
    "    \"\"\"\n",
    "    x2_y2 = x ** 2 + y ** 2\n",
    "    r = np.sqrt(x2_y2 + z ** 2)  # r\n",
    "    elev = np.arctan2(z, np.sqrt(x2_y2))  # Elevation\n",
    "    az = np.arctan2(y, x)  # Azimuth\n",
    "    return r, elev, az\n",
    "\n",
    "\n",
    "def pol2cart(theta, rho):\n",
    "    \"\"\"\n",
    "    Transform polar coordinates to Cartesian\n",
    "    :param theta: angle value\n",
    "    :param rho: radius value\n",
    "    :return: X, Y\n",
    "    \"\"\"\n",
    "    return rho * np.cos(theta), rho * np.sin(theta)\n",
    "\n",
    "#########################################################\n",
    "def load_data(filename='ucieeg.mat'):\n",
    "    # load the .mat file\n",
    "    data_mat = loadmat(filename)\n",
    "    # create separate arrays for each feature/label\n",
    "    data_time = data_mat['X']\n",
    "    label_alcoholism = data_mat['y_alcoholic'].reshape(-1)\n",
    "    label_stimulus = data_mat['y_stimulus'].reshape(-1)\n",
    "    label_id = data_mat['subjectid'].reshape(-1)\n",
    "\n",
    "    # create dataframe of labels\n",
    "    labels = {\n",
    "        'alcoholism': label_alcoholism,\n",
    "        'stimulus': label_stimulus,\n",
    "        'id': label_id\n",
    "    }\n",
    "    df = pd.DataFrame(labels)\n",
    "\n",
    "    # remove all trials containing stimulus 3 and 4 (data)\n",
    "    indices_stimulus_4 = df.loc[(df['stimulus'] == 4)].index\n",
    "    indices_stimulus_5 = df.loc[(df['stimulus'] == 5)].index\n",
    "    indices_stimulus_4_n_5 = np.array(indices_stimulus_4.append(indices_stimulus_5))\n",
    "    data_time = np.delete(data_time, indices_stimulus_4_n_5, 0)\n",
    "\n",
    "    # remove all trials containing stimulus 3 and 4 (labels)\n",
    "    num_samples = len(data_time)\n",
    "    indices_new = pd.Series(np.arange(0, num_samples))\n",
    "    df = df.drop(indices_stimulus_4_n_5, axis=0).set_index(indices_new)\n",
    "    return data_time, df\n",
    "\n",
    "\n",
    "def get_average_data(data_input, window_size):\n",
    "    '''\n",
    "        Returns the average of the data_input over the window_size\n",
    "    '''\n",
    "    data_avg_list = []\n",
    "    for index in range(window_size // 2, len(data_input) - window_size + (window_size // 2)):\n",
    "        start = index - window_size // 2\n",
    "        end = index + window_size // 2 + 1\n",
    "\n",
    "        indices = [index_a for index_a in range(start, end)]\n",
    "        data_selected = data_freq[indices]\n",
    "        data_avg = np.mean(data_selected, axis=0)\n",
    "        data_avg_list.append(data_avg)\n",
    "    data_avg_list = np.array(data_avg_list)\n",
    "    return data_avg_list\n",
    "\n",
    "\n",
    "def hide_identity_keep_stimulus(df, data_freq, alcoholism_condition, window_size=3, to_sample=2000, random_state=0):\n",
    "    # V stands for variable alcoholism\n",
    "    to_sample = to_sample // 3\n",
    "    to_sample += window_size\n",
    "\n",
    "    # extract the indices for the three different stimulus conditons with the specifiec alcoholism condition\n",
    "    indices_V_1_T = df.query(f'alcoholism == {alcoholism_condition} and stimulus == 1').index\n",
    "    indices_V_2_T = df.query(f'alcoholism == {alcoholism_condition} and stimulus == 2').index\n",
    "    indices_V_3_T = df.query(f'alcoholism == {alcoholism_condition} and stimulus == 3').index\n",
    "\n",
    "    # extract the corresponding entries in the dataframe\n",
    "    df_V_1_T = df.iloc[indices_V_1_T].sample(n=to_sample, replace=True, random_state=random_state)\n",
    "    df_V_2_T = df.iloc[indices_V_2_T].sample(n=to_sample, replace=True, random_state=random_state)\n",
    "    df_V_3_T = df.iloc[indices_V_3_T].sample(n=to_sample, replace=True, random_state=random_state)\n",
    "\n",
    "    # TODO adapt a better sampling strategy to make sure that people with same identities\n",
    "    # do not end up in a given window size\n",
    "\n",
    "    # average over the identities, while keeping stimulus and alcoholism informaiton\n",
    "    data_V_1_F = get_average_data(df_V_1_T, window_size)\n",
    "    data_V_2_F = get_average_data(df_V_2_T, window_size)\n",
    "    data_V_3_F = get_average_data(df_V_3_T, window_size)\n",
    "\n",
    "    # stack the three different stimulius conditions into one\n",
    "    data_V_T_F = np.vstack((data_V_1_F, data_V_2_F, data_V_3_F))\n",
    "\n",
    "    return data_V_T_F\n",
    "\n",
    "\n",
    "def hide_identity_hide_stimulus(df, data_freq, alcoholism_condition, window_size=3, to_sample=2000, random_state=0):\n",
    "    # V stands for variable alcoholism\n",
    "    to_sample += window_size\n",
    "\n",
    "    # extract the indices corresponding to alcoholism\n",
    "    indices_V_T_T = df.query(f'alcoholism == {alcoholism_condition}').index\n",
    "\n",
    "    # extract the corresponding entries in the dataframe\n",
    "    df_V_T_T = df.iloc[indices_V_T_T].sample(n=to_sample, replace=True, random_state=random_state)\n",
    "\n",
    "    # TODO adapt a better sampling strategy to make sure that people with same identities\n",
    "    # do not end up in a given window size\n",
    "\n",
    "    # average over the identities and stimulus while keeping alcoholism informaiton\n",
    "    data_V_F_F = get_average_data(df_V_T_T, window_size)\n",
    "\n",
    "    return data_V_F_F\n",
    "\n",
    "\n",
    "def keep_identity_hide_stimulus(df, data_freq, alcoholism_condition, window_size=3, to_sample=2000, random_state=0):\n",
    "    # V stands for variable alcoholism\n",
    "    to_sample_in = to_sample\n",
    "    if alcoholism_condition == 1:\n",
    "        to_sample = to_sample // 77\n",
    "    else:\n",
    "        to_sample = to_sample // 20\n",
    "    to_sample += window_size\n",
    "\n",
    "    data_V_F_VI_array = []\n",
    "\n",
    "    identity_range = np.arange(1, 123)\n",
    "    for index_id in identity_range:\n",
    "        # extract the indices corresponding to alcoholism\n",
    "        indices_V_T_VI = df.query(f'alcoholism == {alcoholism_condition} and id == {index_id}').index\n",
    "        if len(indices_V_T_VI) > 0:\n",
    "            # extract the corresponding entries in the dataframe\n",
    "            df_V_T_VI = df.iloc[indices_V_T_VI].sample(n=to_sample, replace=True, random_state=random_state)\n",
    "\n",
    "            # average over the stimulus, while keeping identity and alcoholism informaiton\n",
    "            data_V_F_VI = get_average_data(indices_V_T_VI, window_size)\n",
    "            if len(data_V_F_VI_array) == 0:\n",
    "                data_V_F_VI_array = data_V_F_VI\n",
    "            else:\n",
    "                data_V_F_VI_array = np.vstack((data_V_F_VI_array, data_V_F_VI))\n",
    "    if len(data_V_F_VI_array) > to_sample_in:\n",
    "        data_V_F_VI_array = data_V_F_VI_array[np.random.choice(np.arange(to_sample_in), to_sample_in)]\n",
    "\n",
    "    return data_V_F_VI_array\n",
    "\n",
    "\n",
    "def keep_identity_keep_stimulus(df, data_freq, alcoholism_condition, window_size=3, to_sample=2000, random_state=0):\n",
    "    # extract the indices corresponding to alcoholism\n",
    "    indices_V_T_T = df.query(f'alcoholism == {alcoholism_condition}').index\n",
    "    df_V_T_T = df.iloc[indices_V_T_T].sample(n=to_sample, replace=True, random_state=random_state)\n",
    "\n",
    "    indices_V_T_T = df_V_T_T.index\n",
    "    data_V_T_T = data_freq[indices_V_T_T]\n",
    "\n",
    "    return data_V_T_T\n",
    "\n",
    "# load the complete ucieeg dataset\n",
    "data_time, df = load_data('ucieeg.mat')\n",
    "\n",
    "# convert data from time to frequency domain\n",
    "print('Converting from time to frequency domain')\n",
    "data_freq = convert_time_to_frequency(data_time)\n",
    "\n",
    "savemat('ucieeg_freq.mat',  {'data_freq': data_freq, 'df': df})\n",
    "df.to_csv('df.csv', index=False)\n",
    "####################################################\n",
    "\n",
    "\n",
    "# data_mat = loadmat('ucieeg_freq.mat')\n",
    "# data_freq = data_mat['data_freq']\n",
    "# df = pd.read_csv('df.csv')\n",
    "\n",
    "np.random.seed(0)\n",
    "# convert 3d locations of electrodes to 2d\n",
    "locs_2d = get_2d_electrode_locations()\n",
    "\n",
    "# extract unique labels\n",
    "labels_unique_alcoholism = np.array(df['alcoholism'].value_counts().keys())\n",
    "labels_unique_stimulus = np.array(df['stimulus'].value_counts().keys())\n",
    "labels_unique_id = np.sort(np.array(df['id'].value_counts().keys()))\n",
    "\n",
    "window_size = 7\n",
    "to_sample = 3000\n",
    "\n",
    "print('Creating synthetic conditioned data')\n",
    "# 1. (T T T) Alcoholism + stimulus + identity\n",
    "data_T_T_T = keep_identity_keep_stimulus(df, data_freq, alcoholism_condition=1, window_size=window_size, to_sample=to_sample)\n",
    "print(f'T T T : {len(data_T_T_T)}')\n",
    "\n",
    "# 2. (F T T) NO-Alcoholism + stimulus + identity\n",
    "data_F_T_T = keep_identity_keep_stimulus(df, data_freq, alcoholism_condition=0, window_size=window_size, to_sample=to_sample)\n",
    "print(f'F T T : {len(data_F_T_T)}')\n",
    "\n",
    "# 3. (T F F) Alcoholism + NO-stimulus + NO-identity\n",
    "data_T_F_F = hide_identity_hide_stimulus(df, data_freq, alcoholism_condition=1, window_size=window_size, to_sample=to_sample)\n",
    "print(f'T F F : {len(data_T_F_F)}')\n",
    "\n",
    "# 4. (F F F) NO-Alcoholism + NO-stimulus + NO-identity\n",
    "data_F_F_F = hide_identity_hide_stimulus(df, data_freq, alcoholism_condition=0, window_size=window_size, to_sample=to_sample)\n",
    "print(f'F F F : {len(data_F_F_F)}')\n",
    "\n",
    "# 5. (T F T) Alcoholism + NO-stimulus + identity\n",
    "data_T_F_T = keep_identity_hide_stimulus(df, data_freq, alcoholism_condition=1, window_size=window_size, to_sample=to_sample)\n",
    "print(f'T F T : {len(data_T_F_T)}')\n",
    "\n",
    "# 6. (F F T) Alcoholism + NO-stimulus + identity\n",
    "data_F_F_T = keep_identity_hide_stimulus(df, data_freq, alcoholism_condition=0, window_size=window_size, to_sample=to_sample)\n",
    "print(f'F F T : {len(data_F_F_T)}')\n",
    "\n",
    "# 7. (T T F) Alcoholism + stimulus + NO-identity\n",
    "data_T_T_F = hide_identity_keep_stimulus(df, data_freq, alcoholism_condition=1, window_size=5, to_sample=to_sample)\n",
    "print(f'T T F : {len(data_T_T_F)}')\n",
    "\n",
    "# 8. (F T F) NO-Alcoholism + stimulus + NO-identity\n",
    "data_F_T_F = hide_identity_keep_stimulus(df, data_freq, alcoholism_condition=0, window_size=5, to_sample=to_sample)\n",
    "print(f'F T F : {len(data_F_T_F)}')\n",
    "\n",
    "print('Making frames')\n",
    "# create frames (extracts only the theta, aplha and beta channels (3))\n",
    "frames_T_T_T = make_frames(data_T_T_T, 1)\n",
    "frames_F_T_T = make_frames(data_F_T_T, 1)\n",
    "frames_T_F_F = make_frames(data_T_F_F, 1)\n",
    "frames_F_F_F = make_frames(data_F_F_F, 1)\n",
    "frames_T_F_T = make_frames(data_T_F_T, 1)\n",
    "frames_F_F_T = make_frames(data_F_F_T, 1)\n",
    "frames_T_T_F = make_frames(data_T_T_F, 1)\n",
    "frames_F_T_F = make_frames(data_F_T_F, 1)\n",
    "\n",
    "# convert signals to RGB images\n",
    "print('Generating images from signals')\n",
    "images_T_T_T = gen_images(locs_2d, frames_T_T_T, 32, normalize=False)\n",
    "images_F_T_T = gen_images(locs_2d, frames_F_T_T, 32, normalize=False)\n",
    "images_T_F_F = gen_images(locs_2d, frames_T_F_F, 32, normalize=False)\n",
    "images_F_F_F = gen_images(locs_2d, frames_F_F_F, 32, normalize=False)\n",
    "images_T_F_T = gen_images(locs_2d, frames_T_F_T, 32, normalize=False)\n",
    "images_F_F_T = gen_images(locs_2d, frames_F_F_T, 32, normalize=False)\n",
    "images_T_T_F = gen_images(locs_2d, frames_T_T_F, 32, normalize=False)\n",
    "images_F_T_F = gen_images(locs_2d, frames_F_T_F, 32, normalize=False)\n",
    "\n",
    "# print('Displaying images')\n",
    "# for i in range(5):\n",
    "#     plt.figure()\n",
    "#     plt.imshow(images_F_F_F[i])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_T_T_T\n",
    "# images_F_T_T\n",
    "# images_T_F_F\n",
    "# images_F_F_F\n",
    "# images_T_F_T\n",
    "# images_F_F_T\n",
    "# images_T_T_F\n",
    "# images_F_T_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.vstack((images_T_T_T, images_F_T_T, images_T_F_F, images_F_F_F, images_T_F_T, images_F_F_T, images_T_T_F, images_F_T_F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_T_T_T = np.repeat(np.array([[1, 1, 1]]), len(images_T_T_T), axis=0)\n",
    "labels_F_T_T = np.repeat(np.array([[0, 1, 1]]), len(images_F_T_T), axis=0)\n",
    "labels_T_F_F = np.repeat(np.array([[1, 0, 0]]), len(images_T_F_F), axis=0)\n",
    "labels_F_F_F = np.repeat(np.array([[0, 0, 0]]), len(images_F_F_F), axis=0)\n",
    "labels_T_F_T = np.repeat(np.array([[1, 0, 1]]), len(images_T_F_T), axis=0)\n",
    "labels_F_F_T = np.repeat(np.array([[0, 0, 1]]), len(images_F_F_T), axis=0)\n",
    "labels_T_T_F = np.repeat(np.array([[1, 1, 0]]), len(images_T_T_F), axis=0)\n",
    "labels_F_T_F = np.repeat(np.array([[0, 1, 0]]), len(images_F_T_F), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.vstack((labels_T_T_T, labels_F_T_T, labels_T_F_F, labels_F_F_F, labels_T_F_T, labels_F_F_T, labels_T_T_F, labels_F_T_F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving images')\n",
    "savemat('conditional_data.mat',\n",
    "        {\n",
    "            'images': images,\n",
    "            'labels': labels\n",
    "         })\n",
    "\n",
    "print('Save successful!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
